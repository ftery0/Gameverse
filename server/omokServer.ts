import WebSocket, { WebSocketServer } from 'ws';
import { LLM } from 'llama-node';
import { LLamaCpp } from 'llama-node/dist/llm/llama-cpp.js';
import path from 'path';

const wss = new WebSocketServer({ port: 8080 });
const llama = new LLM(LLamaCpp);

let isModelLoaded = false;

async function loadModel() {
  if (!isModelLoaded) {
    await llama.load({
        modelPath: path.resolve(process.cwd(), 'model/ggml-model.bin'),
        enableLogging: false,
        nCtx: 2048,
        seed: 1337,
        nGpuLayers: 0,
        f16Kv: false,
        logitsAll: false,
        vocabOnly: false,
        useMlock: false,
        embedding: false,
        useMmap: false
    });
    isModelLoaded = true;
    console.log('ğŸ§  LLaMA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ');
  }
}

wss.on('connection', (ws: WebSocket) => {
  console.log('âœ… í´ë¼ì´ì–¸íŠ¸ ì ‘ì†ë¨');

  ws.on('message', async (message: WebSocket.RawData) => {
    const parsed = JSON.parse(message.toString());
    const { boardState } = parsed;

    await loadModel();

    const prompt = `
ë‹¹ì‹ ì€ ì˜¤ëª© AIì…ë‹ˆë‹¤.
í˜„ì¬ ë³´ë“œ ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ (15x15 ë°°ì—´ì…ë‹ˆë‹¤):

${boardState.map((row: any[]) => row.join(' ')).join('\n')}

ë‹¹ì‹ ì€ í‘ëŒì…ë‹ˆë‹¤. ë‹¤ìŒ ìˆ˜ë¥¼ (x, y) ì¢Œí‘œë¡œ ì‘ë‹µí•˜ì„¸ìš”.
ì‘ë‹µ í˜•ì‹: {"x": 7, "y": 8"}
`;

    const params = {
      nThreads: 4,
      nTokPredict: 50,
      topK: 40,
      topP: 0.9,
      temp: 0.7,
      repeatPenalty: 1,
      prompt,
    };

    let result = '';

    await llama.createCompletion(params, (response) => {
      result += response.token;
    });

    try {
      const move = JSON.parse(result.trim());
      ws.send(JSON.stringify({ type: 'aiMove', move }));
    } catch (err) {
      console.error('âŒ AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜:', err);
      ws.send(JSON.stringify({ type: 'error', message: 'AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨' }));
    }
  });
});

console.log('ğŸ§  ì˜¤ëª© WebSocket ì„œë²„ ì‹¤í–‰ ì¤‘: ws://localhost:8080');
